{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:21:50.722104Z",
     "start_time": "2023-05-03T11:21:42.025969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import  spacy\n",
    "from spacy.tokens import DocBin\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:21:55.013252Z",
     "start_time": "2023-05-03T11:21:50.730917Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:50.043321Z",
     "start_time": "2023-05-03T11:21:55.013252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"subtitles_raw/en_raw_0-900.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    en_all = [line.strip() for line in f]\n",
    "\n",
    "with open(\"subtitles_raw/ru_raw_0-900.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    ru_all = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/ru_subtitles_spacy_dump.bin\", \"rb\") as f:\n",
    "    restored_bytes_data = f.read()\n",
    "\n",
    "nlp = spacy.blank(\"ru\")\n",
    "doc_bin = DocBin().from_bytes(restored_bytes_data)\n",
    "ru_all_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:50.063254Z",
     "start_time": "2023-05-03T11:22:50.054072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_common_lemmas(docs, n: int):\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                words.append(token.lemma_.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "def most_common_lemmas_tagged(docs, tag, n: int):\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                if token.pos_ == tag:\n",
    "                    words.append(token.lemma_.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:52.276480Z",
     "start_time": "2023-05-03T11:22:50.064397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('человек', 10941),\n",
       " ('спасибо', 8292),\n",
       " ('дело', 8209),\n",
       " ('время', 7552),\n",
       " ('мистер', 6779),\n",
       " ('день', 6667),\n",
       " ('год', 5763),\n",
       " ('раз', 5616),\n",
       " ('ночь', 5060),\n",
       " ('жизнь', 4963),\n",
       " ('сэр', 4919),\n",
       " ('деньга', 4842),\n",
       " ('отец', 4505),\n",
       " ('дом', 4493),\n",
       " ('место', 4146),\n",
       " ('друг', 4141),\n",
       " ('женщина', 3996),\n",
       " ('ребёнок', 3993),\n",
       " ('вечер', 3844),\n",
       " ('правда', 3706),\n",
       " ('работа', 3619),\n",
       " ('доктор', 3430),\n",
       " ('рука', 3306),\n",
       " ('утро', 3297),\n",
       " ('мисс', 3284),\n",
       " ('жена', 3277),\n",
       " ('мама', 3205),\n",
       " ('что-то', 3181),\n",
       " ('привет', 3166),\n",
       " ('час', 3097),\n",
       " ('девушка', 3038),\n",
       " ('господин', 2969),\n",
       " ('порядок', 2842),\n",
       " ('парень', 2821),\n",
       " ('слово', 2716),\n",
       " ('минута', 2667),\n",
       " ('имя', 2481),\n",
       " ('машина', 2429),\n",
       " ('мир', 2420),\n",
       " ('вещь', 2398),\n",
       " ('мужчина', 2353),\n",
       " ('муж', 2342),\n",
       " ('город', 2237),\n",
       " ('конец', 2207),\n",
       " ('свидание', 2152),\n",
       " ('голова', 2124),\n",
       " ('папа', 2091),\n",
       " ('случай', 2091),\n",
       " ('миссис', 2091),\n",
       " ('вид', 2058),\n",
       " ('вопрос', 1963),\n",
       " ('глаз', 1928),\n",
       " ('мать', 1923),\n",
       " ('любовь', 1922),\n",
       " ('комната', 1874),\n",
       " ('дверь', 1837),\n",
       " ('полиция', 1817),\n",
       " ('бог', 1808),\n",
       " ('мадам', 1766),\n",
       " ('неделя', 1760),\n",
       " ('право', 1759),\n",
       " ('сын', 1757),\n",
       " ('смерть', 1703),\n",
       " ('вода', 1700),\n",
       " ('брат', 1672),\n",
       " ('капитан', 1659),\n",
       " ('сторона', 1625),\n",
       " ('мальчик', 1622),\n",
       " ('лицо', 1512),\n",
       " ('месяц', 1505),\n",
       " ('тысяча', 1502),\n",
       " ('история', 1498),\n",
       " ('ум', 1470),\n",
       " ('дорога', 1469),\n",
       " ('проблема', 1455),\n",
       " ('война', 1434),\n",
       " ('нога', 1398),\n",
       " ('пара', 1374),\n",
       " ('свет', 1355),\n",
       " ('сердце', 1335),\n",
       " ('номер', 1322),\n",
       " ('письмо', 1302),\n",
       " ('семья', 1281),\n",
       " ('помощь', 1268),\n",
       " ('сила', 1255),\n",
       " ('путь', 1252),\n",
       " ('чёрт', 1248),\n",
       " ('сестра', 1237),\n",
       " ('улица', 1234),\n",
       " ('дочь', 1208),\n",
       " ('корабль', 1203),\n",
       " ('земля', 1183),\n",
       " ('кто-то', 1180),\n",
       " ('газета', 1165),\n",
       " ('идея', 1134),\n",
       " ('причина', 1113),\n",
       " ('девочка', 1092),\n",
       " ('доллар', 1037),\n",
       " ('честь', 1014),\n",
       " ('встреча', 1008)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_nouns = most_common_lemmas_tagged(ru_all_docs, \"NOUN\", 100)\n",
    "common_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:52.384747Z",
     "start_time": "2023-05-03T11:22:52.297501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_lines_with_word(original_word: str, docs) -> list:\n",
    "    indexes = []\n",
    "    for i, sentence in enumerate(docs):\n",
    "        for token in sentence:\n",
    "            if token.lemma_.lower() == original_word:\n",
    "                indexes.append(i)\n",
    "                break\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = ['милый', 'отличный', 'ужасный', 'красивый', 'полный', 'маленький', 'странный', 'старый', 'новый', 'нравиться', 'бояться', 'просить', 'позволить', 'считать', 'решить', 'рука', 'история', 'путь', 'место', 'дело', 'случай']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:23:55.869598Z",
     "start_time": "2023-05-03T11:22:52.369120Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53251"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = []\n",
    "for original_word in common:\n",
    "    indexes += find_lines_with_word(original_word, ru_all_docs)\n",
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:23:56.010689Z",
     "start_time": "2023-05-03T11:23:55.869598Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_translated_all = \"corpora/subtitles/translations/opus10_whole.txt\"\n",
    "with open(filename_translated_all, \"rt\", encoding=\"utf-8\") as f:\n",
    "    translated_all = [line.rstrip() for line in f.readlines()]\n",
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T00:59:50.823596Z",
     "start_time": "2023-05-02T23:26:29.895174Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:35<00:00,  1.03s/it]\n",
      "100%|██████████| 500/500 [12:03<00:00,  1.45s/it] \n",
      "100%|██████████| 500/500 [10:25<00:00,  1.25s/it]\n",
      "100%|██████████| 500/500 [12:38<00:00,  1.52s/it]\n",
      "100%|██████████| 500/500 [08:17<00:00,  1.01it/s]\n",
      "100%|██████████| 500/500 [12:29<00:00,  1.50s/it] \n",
      "100%|██████████| 500/500 [10:53<00:00,  1.31s/it]\n",
      "100%|██████████| 500/500 [11:44<00:00,  1.41s/it]\n",
      "100%|██████████| 500/500 [12:52<00:00,  1.54s/it]  \n",
      "100%|██████████| 500/500 [10:12<00:00,  1.22s/it]\n",
      "100%|██████████| 500/500 [09:21<00:00,  1.12s/it]\n",
      "100%|██████████| 500/500 [10:38<00:00,  1.28s/it]\n",
      "100%|██████████| 500/500 [11:26<00:00,  1.37s/it]\n",
      "100%|██████████| 500/500 [13:19<00:00,  1.60s/it]\n",
      "100%|██████████| 500/500 [13:04<00:00,  1.57s/it]\n",
      "100%|██████████| 500/500 [11:55<00:00,  1.43s/it]\n",
      "100%|██████████| 500/500 [12:11<00:00,  1.46s/it]\n",
      "100%|██████████| 500/500 [13:35<00:00,  1.63s/it] \n",
      "100%|██████████| 500/500 [14:03<00:00,  1.69s/it]\n",
      "100%|██████████| 500/500 [12:49<00:00,  1.54s/it] \n",
      "100%|██████████| 500/500 [10:45<00:00,  1.29s/it]\n",
      "100%|██████████| 500/500 [11:02<00:00,  1.32s/it]\n",
      "100%|██████████| 500/500 [11:48<00:00,  1.42s/it]\n",
      "100%|██████████| 500/500 [12:38<00:00,  1.52s/it] \n",
      "100%|██████████| 500/500 [11:18<00:00,  1.36s/it]\n",
      "100%|██████████| 500/500 [12:27<00:00,  1.49s/it]\n",
      "100%|██████████| 500/500 [12:55<00:00,  1.55s/it] \n",
      "100%|██████████| 500/500 [10:07<00:00,  1.21s/it]\n",
      "100%|██████████| 500/500 [10:13<00:00,  1.23s/it]\n",
      "100%|██████████| 500/500 [10:02<00:00,  1.21s/it]\n",
      "100%|██████████| 500/500 [09:17<00:00,  1.12s/it]\n",
      "100%|██████████| 500/500 [09:28<00:00,  1.14s/it]\n",
      "100%|██████████| 500/500 [08:22<00:00,  1.00s/it]\n",
      "100%|██████████| 500/500 [08:43<00:00,  1.05s/it]\n",
      "100%|██████████| 500/500 [08:50<00:00,  1.06s/it]\n",
      "100%|██████████| 500/500 [08:41<00:00,  1.04s/it]\n",
      "100%|██████████| 500/500 [07:51<00:00,  1.06it/s] \n",
      "100%|██████████| 500/500 [09:25<00:00,  1.13s/it]\n",
      "100%|██████████| 500/500 [10:09<00:00,  1.22s/it]\n",
      "100%|██████████| 500/500 [09:16<00:00,  1.11s/it]\n",
      "100%|██████████| 500/500 [09:11<00:00,  1.10s/it]\n",
      "100%|██████████| 500/500 [09:26<00:00,  1.13s/it]\n",
      "100%|██████████| 500/500 [10:40<00:00,  1.28s/it]\n",
      "100%|██████████| 500/500 [12:25<00:00,  1.49s/it]\n",
      "100%|██████████| 500/500 [16:29<00:00,  1.98s/it] \n",
      "100%|██████████| 500/500 [16:39<00:00,  2.00s/it]\n",
      "100%|██████████| 500/500 [14:45<00:00,  1.77s/it]\n",
      "100%|██████████| 500/500 [15:34<00:00,  1.87s/it] \n",
      "100%|██████████| 500/500 [18:22<00:00,  2.21s/it]  \n",
      "100%|██████████| 500/500 [14:38<00:00,  1.76s/it] \n",
      "100%|██████████| 500/500 [16:42<00:00,  2.01s/it]\n",
      "100%|██████████| 500/500 [14:14<00:00,  1.71s/it] \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "latest_chunk = 1000\n",
    "\n",
    "for chunk_start in range(latest_chunk, len(indexes) // 2, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, min(chunk_start + chunk_size, len(indexes)))):\n",
    "        if translated_all[indexes[i]] != \"\":\n",
    "            continue\n",
    "        sentence = ru_all[indexes[i]]\n",
    "        translation = model.translate(sentence, source_lang='ru', target_lang='en', beam_size=10, max_length=200)\n",
    "        translated_all[indexes[i]] = translation\n",
    "\n",
    "    with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in translated_all:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T17:11:05.283645Z",
     "start_time": "2023-05-02T17:11:04.796395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in translated_all:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "выравнивание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "дамп доков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:01:50.632966Z",
     "start_time": "2023-05-03T01:01:48.263688Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_nlp_lg = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:06:57.823556Z",
     "start_time": "2023-05-03T01:06:57.764869Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:41.140262Z",
     "start_time": "2023-05-03T01:08:47.181573Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900000/900000 [15:09<00:00, 990.06it/s] \n",
      "100%|██████████| 900000/900000 [00:22<00:00, 39981.31it/s]\n"
     ]
    }
   ],
   "source": [
    "empty = en_nlp_lg(\"\")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for sentence in tqdm(translated_all):\n",
    "    if sentence == \"\":\n",
    "        docs.append(empty)\n",
    "    elif len(en_translated_docs[i]) != 0:\n",
    "        docs.append(en_translated_docs[i])\n",
    "    else:\n",
    "        spacy_doc = en_nlp_lg(sentence)\n",
    "        docs.append(spacy_doc)\n",
    "\n",
    "doc_bin = DocBin()\n",
    "for doc in tqdm(docs):\n",
    "    doc_bin.add(doc)\n",
    "bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"wb\") as file:\n",
    "    file.write(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_check.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    " \n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:27:03.530103Z",
     "start_time": "2023-05-03T01:27:03.520060Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = \"corpora/subtitles/translations\"\n",
    "fname_mwmf = f\"{folder_name}/mwmf\"\n",
    "fname_itermax = f\"{folder_name}/itermax\"\n",
    "fname_inter = f\"{folder_name}/inter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fname_mwmf, \"rt\", encoding=\"utf-8\") as f:\n",
    "    mwmf = [line.rstrip() for line in f.readlines()]\n",
    "with open(fname_itermax, \"rt\", encoding=\"utf-8\") as f:\n",
    "    itermax = [line.rstrip() for line in f.readlines()]\n",
    "with open(fname_inter, \"rt\", encoding=\"utf-8\") as f:\n",
    "    inter = [line.rstrip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itermax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:53.235642Z",
     "start_time": "2023-05-03T01:10:53.103184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mwmf = [\"\" for _ in range(900000)]\n",
    "itermax = [\"\" for _ in range(900000)]\n",
    "inter = [\"\" for _ in range(900000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:59.567928Z",
     "start_time": "2023-05-03T01:10:59.552275Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translated_all_docs = en_translated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mwmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:12:19.376093Z",
     "start_time": "2023-05-03T01:12:15.219231Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-05-09 14:09:37,401 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:21<00:00, 921.12it/s]  \n",
      "100%|██████████| 20000/20000 [03:11<00:00, 104.19it/s]\n",
      "100%|██████████| 20000/20000 [04:10<00:00, 79.74it/s] \n",
      "100%|██████████| 20000/20000 [03:09<00:00, 105.67it/s]\n",
      "100%|██████████| 20000/20000 [02:57<00:00, 112.36it/s] \n",
      "100%|██████████| 20000/20000 [02:41<00:00, 123.96it/s]\n",
      "100%|██████████| 20000/20000 [03:06<00:00, 107.27it/s]\n",
      "100%|██████████| 20000/20000 [02:41<00:00, 124.22it/s]\n",
      "100%|██████████| 20000/20000 [02:45<00:00, 120.69it/s]\n",
      "100%|██████████| 20000/20000 [02:39<00:00, 125.24it/s]\n",
      "100%|██████████| 20000/20000 [02:26<00:00, 136.98it/s]\n",
      "100%|██████████| 20000/20000 [02:27<00:00, 135.66it/s]\n",
      "100%|██████████| 20000/20000 [02:42<00:00, 123.07it/s]\n",
      "100%|██████████| 20000/20000 [02:10<00:00, 152.90it/s]\n",
      "100%|██████████| 20000/20000 [03:20<00:00, 99.79it/s] \n",
      "100%|██████████| 20000/20000 [03:14<00:00, 102.83it/s]\n",
      "100%|██████████| 20000/20000 [03:08<00:00, 106.13it/s]\n",
      "100%|██████████| 20000/20000 [02:12<00:00, 151.19it/s]\n",
      "100%|██████████| 20000/20000 [02:36<00:00, 127.92it/s]\n",
      "100%|██████████| 20000/20000 [02:11<00:00, 152.65it/s]\n",
      "100%|██████████| 20000/20000 [02:22<00:00, 140.16it/s]\n",
      "100%|██████████| 20000/20000 [02:50<00:00, 117.44it/s]\n",
      "100%|██████████| 20000/20000 [03:06<00:00, 107.05it/s]\n",
      "100%|██████████| 20000/20000 [02:27<00:00, 135.84it/s]\n",
      "100%|██████████| 20000/20000 [02:36<00:00, 127.53it/s]\n",
      "100%|██████████| 20000/20000 [02:17<00:00, 145.06it/s]\n",
      "100%|██████████| 20000/20000 [02:38<00:00, 126.05it/s]\n",
      "100%|██████████| 20000/20000 [03:28<00:00, 95.94it/s] \n",
      "100%|██████████| 20000/20000 [02:50<00:00, 117.01it/s]\n",
      "100%|██████████| 20000/20000 [02:37<00:00, 127.20it/s]\n",
      "100%|██████████| 20000/20000 [02:13<00:00, 149.91it/s]\n",
      "100%|██████████| 20000/20000 [02:35<00:00, 128.91it/s]\n",
      "100%|██████████| 20000/20000 [02:02<00:00, 163.54it/s]\n",
      "100%|██████████| 20000/20000 [02:46<00:00, 120.14it/s]\n",
      "100%|██████████| 20000/20000 [02:11<00:00, 151.89it/s]\n",
      "100%|██████████| 20000/20000 [03:00<00:00, 110.56it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 20000\n",
    "latest_chunk = 180000\n",
    "for chunk_start in range(latest_chunk, 900000, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, chunk_start + chunk_size)):\n",
    "        if translated_all[i] == \"\":\n",
    "            continue\n",
    "        if mwmf[i] != \"\":\n",
    "            continue\n",
    "        ru_tokens = [token.text for token in ru_all_docs[i]]\n",
    "        translated_tokens = [token.text for token in translated_all_docs[i]]\n",
    "        # print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\n",
    "        src, trg = (ru_tokens, translated_tokens)\n",
    "        alignments = myaligner.get_word_aligns(src, trg)\n",
    "        mwmf[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"mwmf\"]])\n",
    "        itermax[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"itermax\"]])\n",
    "        inter[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"inter\"]])\n",
    "\n",
    "    with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in mwmf:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in itermax:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in inter:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "merge two translations from different laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_another_translated_all = \"corpora/subtitles/translations/opus10_whole_case.txt\"\n",
    "with open(filename_another_translated_all, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_translated_all = [line.rstrip() for line in f.readlines()]\n",
    "len(another_translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 \n",
      "71 So when he, uh, asked for help, I couldn't say no.\n",
      "72 \n",
      "73 \n",
      "74 \n",
      "75 Stay where you are!\n",
      "76 \n",
      "77 It's a risky case, but the risk is well paid.\n",
      "78 \n",
      "79 In one week, the fisherman earns more than he earns for two years.\n"
     ]
    }
   ],
   "source": [
    "for i in range(70, 80):\n",
    "    print(i, another_translated_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_folder_name = \"translations_after_req_words/translations\"\n",
    "another_fname_mwmf = f\"{folder_name}/mwmf\"\n",
    "another_fname_itermax = f\"{folder_name}/itermax\"\n",
    "another_fname_inter = f\"{folder_name}/inter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(another_fname_mwmf, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_mwmf = [line.rstrip() for line in f.readlines()]\n",
    "with open(another_fname_itermax, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_itermax = [line.rstrip() for line in f.readlines()]\n",
    "with open(another_fname_inter, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_inter = [line.rstrip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(translated_all)):\n",
    "    if translated_all[i] == \"\" and another_translated_all[i] != \"\":\n",
    "        translated_all[i] = another_translated_all[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_translated_all[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_all[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in translated_all:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mwmf)):\n",
    "    if mwmf[i] == \"\" and another_mwmf[i] != \"\":\n",
    "        mwmf[i] = another_mwmf[i] \n",
    "        itermax[i] = another_itermax[i]\n",
    "        inter[i] = another_inter[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in mwmf:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in itermax:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in inter:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "синк доков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcorpora/subtitles/translations/opus10_spacy_final.bin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     en_translated_bytes_data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mblank(\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m doc_bin \u001b[39m=\u001b[39m DocBin()\u001b[39m.\u001b[39mfrom_bytes(en_translated_bytes_data)\n\u001b[0;32m      6\u001b[0m en_translated_docs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc_bin\u001b[39m.\u001b[39mget_docs(nlp\u001b[39m.\u001b[39mvocab))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "len(en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"translations_after_req_words/translations/opus10_spacy_check2.bin\", \"rb\") as file:\n",
    "    another_en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(another_en_translated_bytes_data)\n",
    "another_en_translated_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "len(another_en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_translated_docs[820])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp_lg = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900000it [00:00, 2368423.31it/s]\n",
      "100%|██████████| 900000/900000 [00:28<00:00, 31274.75it/s]\n"
     ]
    }
   ],
   "source": [
    "empty = en_nlp_lg(\"\")\n",
    "docs = []\n",
    "\n",
    "for i, sentence in tqdm(enumerate(translated_all)):\n",
    "    if sentence == \"\":\n",
    "        docs.append(empty)\n",
    "    elif len(en_translated_docs[i]) != 0:\n",
    "        docs.append(en_translated_docs[i])\n",
    "    else:\n",
    "        spacy_doc = en_nlp_lg(sentence)\n",
    "        docs.append(spacy_doc)\n",
    "\n",
    "doc_bin = DocBin()\n",
    "for doc in tqdm(docs):\n",
    "    doc_bin.add(doc)\n",
    "bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"wb\") as file:\n",
    "    file.write(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(translated_all)):\n",
    "    if translated_all[i] != \"\" and len(docs[i]) == 0:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_all_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:44<00:00, 450.22it/s]  \n",
      "100%|██████████| 20000/20000 [03:28<00:00, 96.07it/s] \n",
      " 38%|███▊      | 7649/20000 [01:40<02:42, 76.20it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m src, trg \u001b[39m=\u001b[39m (ru_tokens, translated_tokens)\n\u001b[1;32m---> 14\u001b[0m alignments \u001b[39m=\u001b[39m myaligner\u001b[39m.\u001b[39;49mget_word_aligns(src, trg)\n\u001b[0;32m     15\u001b[0m mwmf[i] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m alignments[\u001b[39m\"\u001b[39m\u001b[39mmwmf\u001b[39m\u001b[39m\"\u001b[39m]])\n\u001b[0;32m     16\u001b[0m itermax[i] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m alignments[\u001b[39m\"\u001b[39m\u001b[39mitermax\u001b[39m\u001b[39m\"\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\simalign\\simalign.py:209\u001b[0m, in \u001b[0;36mSentenceAligner.get_word_aligns\u001b[1;34m(self, src_sent, trg_sent)\u001b[0m\n\u001b[0;32m    206\u001b[0m \t\u001b[39mfor\u001b[39;00m i, wlist \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(l2_tokens):\n\u001b[0;32m    207\u001b[0m \t\tl2_b2w_map \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m wlist]\n\u001b[1;32m--> 209\u001b[0m vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_loader\u001b[39m.\u001b[39;49mget_embed_list([src_sent, trg_sent])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    210\u001b[0m vectors \u001b[39m=\u001b[39m [vectors[i, :\u001b[39mlen\u001b[39m(bpe_lists[i])] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]]\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\simalign\\simalign.py:65\u001b[0m, in \u001b[0;36mEmbeddingLoader.get_embed_list\u001b[1;34m(self, sent_batch)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m \tinputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(sent_batch, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))[\u001b[39m\"\u001b[39m\u001b[39mhidden_states\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(hidden):\n\u001b[0;32m     67\u001b[0m \t\u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpecified to take embeddings from layer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m}\u001b[39;00m\u001b[39m, but model has only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(hidden)\u001b[39m}\u001b[39;00m\u001b[39m layers.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    496\u001b[0m         hidden_states,\n\u001b[0;32m    497\u001b[0m         attention_mask,\n\u001b[0;32m    498\u001b[0m         head_mask,\n\u001b[0;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[0;32m    428\u001b[0m         head_mask,\n\u001b[0;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    431\u001b[0m         past_key_value,\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[0;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:284\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    275\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    276\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 284\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    286\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chunk_size = 20000\n",
    "latest_chunk = 720000\n",
    "for chunk_start in range(latest_chunk, 800000, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, chunk_start + chunk_size)):\n",
    "        if translated_all[i] == \"\":\n",
    "            continue\n",
    "        if mwmf[i] != \"\":\n",
    "            continue\n",
    "        ru_tokens = [token.text for token in ru_all_docs[i]]\n",
    "        translated_tokens = [token.text for token in translated_all_docs[i]]\n",
    "        # print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\n",
    "        src, trg = (ru_tokens, translated_tokens)\n",
    "        alignments = myaligner.get_word_aligns(src, trg)\n",
    "        mwmf[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"mwmf\"]])\n",
    "        itermax[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"itermax\"]])\n",
    "        inter[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"inter\"]])\n",
    "\n",
    "    with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in mwmf:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in itermax:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in inter:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
