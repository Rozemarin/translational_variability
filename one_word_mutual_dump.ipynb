{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:21:50.722104Z",
     "start_time": "2023-05-03T11:21:42.025969Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import  spacy\n",
    "from spacy.tokens import DocBin\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:21:55.013252Z",
     "start_time": "2023-05-03T11:21:50.730917Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:50.043321Z",
     "start_time": "2023-05-03T11:21:55.013252Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"subtitles_raw/en_raw_0-900.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    en_all = [line.strip() for line in f]\n",
    "\n",
    "with open(\"subtitles_raw/ru_raw_0-900.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "    ru_all = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/ru_subtitles_spacy_dump.bin\", \"rb\") as f:\n",
    "    restored_bytes_data = f.read()\n",
    "\n",
    "nlp = spacy.blank(\"ru\")\n",
    "doc_bin = DocBin().from_bytes(restored_bytes_data)\n",
    "ru_all_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:50.063254Z",
     "start_time": "2023-05-03T11:22:50.054072Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_common_lemmas(docs, n: int):\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                words.append(token.lemma_.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "def most_common_lemmas_tagged(docs, tag, n: int):\n",
    "    words = []\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if not token.is_stop and not token.is_punct:\n",
    "                if token.pos_ == tag:\n",
    "                    words.append(token.lemma_.lower())\n",
    "    word_freq = Counter(words)\n",
    "    return word_freq.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:52.276480Z",
     "start_time": "2023-05-03T11:22:50.064397Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('человек', 10941),\n",
       " ('спасибо', 8292),\n",
       " ('дело', 8209),\n",
       " ('время', 7552),\n",
       " ('мистер', 6779),\n",
       " ('день', 6667),\n",
       " ('год', 5763),\n",
       " ('раз', 5616),\n",
       " ('ночь', 5060),\n",
       " ('жизнь', 4963),\n",
       " ('сэр', 4919),\n",
       " ('деньга', 4842),\n",
       " ('отец', 4505),\n",
       " ('дом', 4493),\n",
       " ('место', 4146),\n",
       " ('друг', 4141),\n",
       " ('женщина', 3996),\n",
       " ('ребёнок', 3993),\n",
       " ('вечер', 3844),\n",
       " ('правда', 3706),\n",
       " ('работа', 3619),\n",
       " ('доктор', 3430),\n",
       " ('рука', 3306),\n",
       " ('утро', 3297),\n",
       " ('мисс', 3284),\n",
       " ('жена', 3277),\n",
       " ('мама', 3205),\n",
       " ('что-то', 3181),\n",
       " ('привет', 3166),\n",
       " ('час', 3097),\n",
       " ('девушка', 3038),\n",
       " ('господин', 2969),\n",
       " ('порядок', 2842),\n",
       " ('парень', 2821),\n",
       " ('слово', 2716),\n",
       " ('минута', 2667),\n",
       " ('имя', 2481),\n",
       " ('машина', 2429),\n",
       " ('мир', 2420),\n",
       " ('вещь', 2398),\n",
       " ('мужчина', 2353),\n",
       " ('муж', 2342),\n",
       " ('город', 2237),\n",
       " ('конец', 2207),\n",
       " ('свидание', 2152),\n",
       " ('голова', 2124),\n",
       " ('папа', 2091),\n",
       " ('случай', 2091),\n",
       " ('миссис', 2091),\n",
       " ('вид', 2058),\n",
       " ('вопрос', 1963),\n",
       " ('глаз', 1928),\n",
       " ('мать', 1923),\n",
       " ('любовь', 1922),\n",
       " ('комната', 1874),\n",
       " ('дверь', 1837),\n",
       " ('полиция', 1817),\n",
       " ('бог', 1808),\n",
       " ('мадам', 1766),\n",
       " ('неделя', 1760),\n",
       " ('право', 1759),\n",
       " ('сын', 1757),\n",
       " ('смерть', 1703),\n",
       " ('вода', 1700),\n",
       " ('брат', 1672),\n",
       " ('капитан', 1659),\n",
       " ('сторона', 1625),\n",
       " ('мальчик', 1622),\n",
       " ('лицо', 1512),\n",
       " ('месяц', 1505),\n",
       " ('тысяча', 1502),\n",
       " ('история', 1498),\n",
       " ('ум', 1470),\n",
       " ('дорога', 1469),\n",
       " ('проблема', 1455),\n",
       " ('война', 1434),\n",
       " ('нога', 1398),\n",
       " ('пара', 1374),\n",
       " ('свет', 1355),\n",
       " ('сердце', 1335),\n",
       " ('номер', 1322),\n",
       " ('письмо', 1302),\n",
       " ('семья', 1281),\n",
       " ('помощь', 1268),\n",
       " ('сила', 1255),\n",
       " ('путь', 1252),\n",
       " ('чёрт', 1248),\n",
       " ('сестра', 1237),\n",
       " ('улица', 1234),\n",
       " ('дочь', 1208),\n",
       " ('корабль', 1203),\n",
       " ('земля', 1183),\n",
       " ('кто-то', 1180),\n",
       " ('газета', 1165),\n",
       " ('идея', 1134),\n",
       " ('причина', 1113),\n",
       " ('девочка', 1092),\n",
       " ('доллар', 1037),\n",
       " ('честь', 1014),\n",
       " ('встреча', 1008)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_nouns = most_common_lemmas_tagged(ru_all_docs, \"NOUN\", 100)\n",
    "common_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:22:52.384747Z",
     "start_time": "2023-05-03T11:22:52.297501Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_lines_with_word(original_word: str, docs) -> list:\n",
    "    indexes = []\n",
    "    for i, sentence in enumerate(docs):\n",
    "        for token in sentence:\n",
    "            if token.lemma_.lower() == original_word:\n",
    "                indexes.append(i)\n",
    "                break\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "common = ['милый', 'отличный', 'ужасный', 'красивый', 'полный', 'маленький', 'странный', 'старый', 'новый', 'нравиться', 'бояться', 'просить', 'позволить', 'считать', 'решить', 'рука', 'история', 'путь', 'место', 'дело', 'случай']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:23:55.869598Z",
     "start_time": "2023-05-03T11:22:52.369120Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53251"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = []\n",
    "for original_word in common:\n",
    "    indexes += find_lines_with_word(original_word, ru_all_docs)\n",
    "len(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T11:23:56.010689Z",
     "start_time": "2023-05-03T11:23:55.869598Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_translated_all = \"corpora/subtitles/translations/opus10_whole.txt\"\n",
    "with open(filename_translated_all, \"rt\", encoding=\"utf-8\") as f:\n",
    "    translated_all = [line.rstrip() for line in f.readlines()]\n",
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T00:59:50.823596Z",
     "start_time": "2023-05-02T23:26:29.895174Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [08:35<00:00,  1.03s/it]\n",
      "100%|██████████| 500/500 [12:03<00:00,  1.45s/it] \n",
      "100%|██████████| 500/500 [10:25<00:00,  1.25s/it]\n",
      "100%|██████████| 500/500 [12:38<00:00,  1.52s/it]\n",
      "100%|██████████| 500/500 [08:17<00:00,  1.01it/s]\n",
      "100%|██████████| 500/500 [12:29<00:00,  1.50s/it] \n",
      "100%|██████████| 500/500 [10:53<00:00,  1.31s/it]\n",
      "100%|██████████| 500/500 [11:44<00:00,  1.41s/it]\n",
      "100%|██████████| 500/500 [12:52<00:00,  1.54s/it]  \n",
      "100%|██████████| 500/500 [10:12<00:00,  1.22s/it]\n",
      "100%|██████████| 500/500 [09:21<00:00,  1.12s/it]\n",
      "100%|██████████| 500/500 [10:38<00:00,  1.28s/it]\n",
      "100%|██████████| 500/500 [11:26<00:00,  1.37s/it]\n",
      "100%|██████████| 500/500 [13:19<00:00,  1.60s/it]\n",
      "100%|██████████| 500/500 [13:04<00:00,  1.57s/it]\n",
      "100%|██████████| 500/500 [11:55<00:00,  1.43s/it]\n",
      "100%|██████████| 500/500 [12:11<00:00,  1.46s/it]\n",
      "100%|██████████| 500/500 [13:35<00:00,  1.63s/it] \n",
      "100%|██████████| 500/500 [14:03<00:00,  1.69s/it]\n",
      "100%|██████████| 500/500 [12:49<00:00,  1.54s/it] \n",
      "100%|██████████| 500/500 [10:45<00:00,  1.29s/it]\n",
      "100%|██████████| 500/500 [11:02<00:00,  1.32s/it]\n",
      "100%|██████████| 500/500 [11:48<00:00,  1.42s/it]\n",
      "100%|██████████| 500/500 [12:38<00:00,  1.52s/it] \n",
      "100%|██████████| 500/500 [11:18<00:00,  1.36s/it]\n",
      "100%|██████████| 500/500 [12:27<00:00,  1.49s/it]\n",
      "100%|██████████| 500/500 [12:55<00:00,  1.55s/it] \n",
      "100%|██████████| 500/500 [10:07<00:00,  1.21s/it]\n",
      "100%|██████████| 500/500 [10:13<00:00,  1.23s/it]\n",
      "100%|██████████| 500/500 [10:02<00:00,  1.21s/it]\n",
      "100%|██████████| 500/500 [09:17<00:00,  1.12s/it]\n",
      "100%|██████████| 500/500 [09:28<00:00,  1.14s/it]\n",
      "100%|██████████| 500/500 [08:22<00:00,  1.00s/it]\n",
      "100%|██████████| 500/500 [08:43<00:00,  1.05s/it]\n",
      "100%|██████████| 500/500 [08:50<00:00,  1.06s/it]\n",
      "100%|██████████| 500/500 [08:41<00:00,  1.04s/it]\n",
      "100%|██████████| 500/500 [07:51<00:00,  1.06it/s] \n",
      "100%|██████████| 500/500 [09:25<00:00,  1.13s/it]\n",
      "100%|██████████| 500/500 [10:09<00:00,  1.22s/it]\n",
      "100%|██████████| 500/500 [09:16<00:00,  1.11s/it]\n",
      "100%|██████████| 500/500 [09:11<00:00,  1.10s/it]\n",
      "100%|██████████| 500/500 [09:26<00:00,  1.13s/it]\n",
      "100%|██████████| 500/500 [10:40<00:00,  1.28s/it]\n",
      "100%|██████████| 500/500 [12:25<00:00,  1.49s/it]\n",
      "100%|██████████| 500/500 [16:29<00:00,  1.98s/it] \n",
      "100%|██████████| 500/500 [16:39<00:00,  2.00s/it]\n",
      "100%|██████████| 500/500 [14:45<00:00,  1.77s/it]\n",
      "100%|██████████| 500/500 [15:34<00:00,  1.87s/it] \n",
      "100%|██████████| 500/500 [18:22<00:00,  2.21s/it]  \n",
      "100%|██████████| 500/500 [14:38<00:00,  1.76s/it] \n",
      "100%|██████████| 500/500 [16:42<00:00,  2.01s/it]\n",
      "100%|██████████| 500/500 [14:14<00:00,  1.71s/it] \n"
     ]
    }
   ],
   "source": [
    "chunk_size = 500\n",
    "latest_chunk = 1000\n",
    "\n",
    "for chunk_start in range(latest_chunk, len(indexes) // 2, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, min(chunk_start + chunk_size, len(indexes)))):\n",
    "        if translated_all[indexes[i]] != \"\":\n",
    "            continue\n",
    "        sentence = ru_all[indexes[i]]\n",
    "        translation = model.translate(sentence, source_lang='ru', target_lang='en', beam_size=10, max_length=200)\n",
    "        translated_all[indexes[i]] = translation\n",
    "\n",
    "    with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in translated_all:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-02T17:11:05.283645Z",
     "start_time": "2023-05-02T17:11:04.796395Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in translated_all:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "выравнивание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "дамп доков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:01:50.632966Z",
     "start_time": "2023-05-03T01:01:48.263688Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "en_nlp_lg = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:06:57.823556Z",
     "start_time": "2023-05-03T01:06:57.764869Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:41.140262Z",
     "start_time": "2023-05-03T01:08:47.181573Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900000/900000 [15:09<00:00, 990.06it/s] \n",
      "100%|██████████| 900000/900000 [00:22<00:00, 39981.31it/s]\n"
     ]
    }
   ],
   "source": [
    "empty = en_nlp_lg(\"\")\n",
    "\n",
    "docs = []\n",
    "\n",
    "for sentence in tqdm(translated_all):\n",
    "    if sentence == \"\":\n",
    "        docs.append(empty)\n",
    "    elif len(en_translated_docs[i]) != 0:\n",
    "        docs.append(en_translated_docs[i])\n",
    "    else:\n",
    "        spacy_doc = en_nlp_lg(sentence)\n",
    "        docs.append(spacy_doc)\n",
    "\n",
    "doc_bin = DocBin()\n",
    "for doc in tqdm(docs):\n",
    "    doc_bin.add(doc)\n",
    "bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"wb\") as file:\n",
    "    file.write(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_translated_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    " \n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:27:03.530103Z",
     "start_time": "2023-05-03T01:27:03.520060Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder_name = \"corpora/subtitles/translations\"\n",
    "fname_mwmf = f\"{folder_name}/mwmf\"\n",
    "fname_itermax = f\"{folder_name}/itermax\"\n",
    "fname_inter = f\"{folder_name}/inter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(fname_mwmf, \"rt\", encoding=\"utf-8\") as f:\n",
    "    mwmf = [line.rstrip() for line in f.readlines()]\n",
    "with open(fname_itermax, \"rt\", encoding=\"utf-8\") as f:\n",
    "    itermax = [line.rstrip() for line in f.readlines()]\n",
    "with open(fname_inter, \"rt\", encoding=\"utf-8\") as f:\n",
    "    inter = [line.rstrip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:53.235642Z",
     "start_time": "2023-05-03T01:10:53.103184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mwmf = [\"\" for _ in range(900000)]\n",
    "itermax = [\"\" for _ in range(900000)]\n",
    "inter = [\"\" for _ in range(900000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:10:59.567928Z",
     "start_time": "2023-05-03T01:10:59.552275Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translated_all_docs = en_translated_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mwmf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T01:12:19.376093Z",
     "start_time": "2023-05-03T01:12:15.219231Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2023-05-09 14:09:37,401 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:21<00:00, 921.12it/s]  \n",
      "100%|██████████| 20000/20000 [03:11<00:00, 104.19it/s]\n",
      "100%|██████████| 20000/20000 [04:10<00:00, 79.74it/s] \n",
      "100%|██████████| 20000/20000 [03:09<00:00, 105.67it/s]\n",
      "100%|██████████| 20000/20000 [02:57<00:00, 112.36it/s] \n",
      "100%|██████████| 20000/20000 [02:41<00:00, 123.96it/s]\n",
      "100%|██████████| 20000/20000 [03:06<00:00, 107.27it/s]\n",
      "100%|██████████| 20000/20000 [02:41<00:00, 124.22it/s]\n",
      "100%|██████████| 20000/20000 [02:45<00:00, 120.69it/s]\n",
      "100%|██████████| 20000/20000 [02:39<00:00, 125.24it/s]\n",
      "100%|██████████| 20000/20000 [02:26<00:00, 136.98it/s]\n",
      "100%|██████████| 20000/20000 [02:27<00:00, 135.66it/s]\n",
      "100%|██████████| 20000/20000 [02:42<00:00, 123.07it/s]\n",
      "100%|██████████| 20000/20000 [02:10<00:00, 152.90it/s]\n",
      "100%|██████████| 20000/20000 [03:20<00:00, 99.79it/s] \n",
      "100%|██████████| 20000/20000 [03:14<00:00, 102.83it/s]\n",
      "100%|██████████| 20000/20000 [03:08<00:00, 106.13it/s]\n",
      "100%|██████████| 20000/20000 [02:12<00:00, 151.19it/s]\n",
      "100%|██████████| 20000/20000 [02:36<00:00, 127.92it/s]\n",
      "100%|██████████| 20000/20000 [02:11<00:00, 152.65it/s]\n",
      "100%|██████████| 20000/20000 [02:22<00:00, 140.16it/s]\n",
      "100%|██████████| 20000/20000 [02:50<00:00, 117.44it/s]\n",
      "100%|██████████| 20000/20000 [03:06<00:00, 107.05it/s]\n",
      "100%|██████████| 20000/20000 [02:27<00:00, 135.84it/s]\n",
      "100%|██████████| 20000/20000 [02:36<00:00, 127.53it/s]\n",
      "100%|██████████| 20000/20000 [02:17<00:00, 145.06it/s]\n",
      "100%|██████████| 20000/20000 [02:38<00:00, 126.05it/s]\n",
      "100%|██████████| 20000/20000 [03:28<00:00, 95.94it/s] \n",
      "100%|██████████| 20000/20000 [02:50<00:00, 117.01it/s]\n",
      "100%|██████████| 20000/20000 [02:37<00:00, 127.20it/s]\n",
      "100%|██████████| 20000/20000 [02:13<00:00, 149.91it/s]\n",
      "100%|██████████| 20000/20000 [02:35<00:00, 128.91it/s]\n",
      "100%|██████████| 20000/20000 [02:02<00:00, 163.54it/s]\n",
      "100%|██████████| 20000/20000 [02:46<00:00, 120.14it/s]\n",
      "100%|██████████| 20000/20000 [02:11<00:00, 151.89it/s]\n",
      "100%|██████████| 20000/20000 [03:00<00:00, 110.56it/s]\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 20000\n",
    "latest_chunk = 180000\n",
    "for chunk_start in range(latest_chunk, 900000, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, chunk_start + chunk_size)):\n",
    "        if translated_all[i] == \"\":\n",
    "            continue\n",
    "        if mwmf[i] != \"\":\n",
    "            continue\n",
    "        ru_tokens = [token.text for token in ru_all_docs[i]]\n",
    "        translated_tokens = [token.text for token in translated_all_docs[i]]\n",
    "        # print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\n",
    "        src, trg = (ru_tokens, translated_tokens)\n",
    "        alignments = myaligner.get_word_aligns(src, trg)\n",
    "        mwmf[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"mwmf\"]])\n",
    "        itermax[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"itermax\"]])\n",
    "        inter[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"inter\"]])\n",
    "\n",
    "    with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in mwmf:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in itermax:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in inter:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "merge two translations from different laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_another_translated_all = \"corpora/subtitles/translations/opus10_whole_case.txt\"\n",
    "with open(filename_another_translated_all, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_translated_all = [line.rstrip() for line in f.readlines()]\n",
    "len(another_translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 \n",
      "71 So when he, uh, asked for help, I couldn't say no.\n",
      "72 \n",
      "73 \n",
      "74 \n",
      "75 Stay where you are!\n",
      "76 \n",
      "77 It's a risky case, but the risk is well paid.\n",
      "78 \n",
      "79 In one week, the fisherman earns more than he earns for two years.\n"
     ]
    }
   ],
   "source": [
    "for i in range(70, 80):\n",
    "    print(i, another_translated_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_folder_name = \"tmp_alignment\"\n",
    "another_fname_mwmf = f\"{another_folder_name}/mwmf (2)\"\n",
    "another_fname_itermax = f\"{another_folder_name}/itermax (2)\"\n",
    "another_fname_inter = f\"{another_folder_name}/inter (2)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(another_fname_mwmf, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_mwmf = [line.rstrip() for line in f.readlines()]\n",
    "with open(another_fname_itermax, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_itermax = [line.rstrip() for line in f.readlines()]\n",
    "with open(another_fname_inter, \"rt\", encoding=\"utf-8\") as f:\n",
    "    another_inter = [line.rstrip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(translated_all)):\n",
    "    if translated_all[i] == \"\" and another_translated_all[i] != \"\":\n",
    "        translated_all[i] = another_translated_all[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_translation = []\n",
    "lost_alignment = []\n",
    "for i in range(len(translated_all)):\n",
    "    if len(translated_all[i]) == 0 and len(mwmf[i]) != 0:\n",
    "        lost_translation.append(i)\n",
    "    elif  len(translated_all[i]) != 0 and len(mwmf[i]) == 0:\n",
    "        lost_alignment.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100146,\n",
       " 100187,\n",
       " 100189,\n",
       " 100195,\n",
       " 100199,\n",
       " 100251,\n",
       " 100337,\n",
       " 100422,\n",
       " 100665,\n",
       " 100667,\n",
       " 100694,\n",
       " 100708,\n",
       " 100749,\n",
       " 100753,\n",
       " 100789,\n",
       " 100816,\n",
       " 100975,\n",
       " 101002,\n",
       " 101040,\n",
       " 101099,\n",
       " 101147,\n",
       " 101224,\n",
       " 101336,\n",
       " 101488,\n",
       " 101551,\n",
       " 101555,\n",
       " 101709,\n",
       " 101710,\n",
       " 101926,\n",
       " 101927,\n",
       " 102035,\n",
       " 102036,\n",
       " 102083,\n",
       " 102157,\n",
       " 102169,\n",
       " 102173,\n",
       " 102249,\n",
       " 102261,\n",
       " 102300,\n",
       " 102450,\n",
       " 102533,\n",
       " 102581,\n",
       " 102664,\n",
       " 102670,\n",
       " 102684,\n",
       " 102703,\n",
       " 102705,\n",
       " 102736,\n",
       " 102756,\n",
       " 102761,\n",
       " 102795,\n",
       " 102808,\n",
       " 102845,\n",
       " 102886,\n",
       " 102925,\n",
       " 102989,\n",
       " 103034,\n",
       " 103066,\n",
       " 103221,\n",
       " 103264,\n",
       " 103279,\n",
       " 103296,\n",
       " 103355,\n",
       " 103393,\n",
       " 103747,\n",
       " 103756,\n",
       " 103766,\n",
       " 103947,\n",
       " 103953,\n",
       " 103969,\n",
       " 103971,\n",
       " 104012,\n",
       " 104033,\n",
       " 104039,\n",
       " 104158,\n",
       " 104226,\n",
       " 104354,\n",
       " 104361,\n",
       " 104434,\n",
       " 104436,\n",
       " 104452,\n",
       " 104486,\n",
       " 104489,\n",
       " 104509,\n",
       " 104689,\n",
       " 104690,\n",
       " 104691,\n",
       " 104726,\n",
       " 104811,\n",
       " 104820,\n",
       " 104834,\n",
       " 105002,\n",
       " 105004,\n",
       " 105019,\n",
       " 105025,\n",
       " 105037,\n",
       " 105075,\n",
       " 105110,\n",
       " 105127,\n",
       " 105131,\n",
       " 105195,\n",
       " 105217,\n",
       " 105228,\n",
       " 105229,\n",
       " 105238,\n",
       " 105274,\n",
       " 105290,\n",
       " 105302,\n",
       " 105327,\n",
       " 105344,\n",
       " 105420,\n",
       " 105449,\n",
       " 105470,\n",
       " 105604,\n",
       " 105605,\n",
       " 105608,\n",
       " 105618,\n",
       " 105658,\n",
       " 105660,\n",
       " 105669,\n",
       " 105671,\n",
       " 105676,\n",
       " 105688,\n",
       " 105694,\n",
       " 105729,\n",
       " 105730,\n",
       " 105777,\n",
       " 105783,\n",
       " 105849,\n",
       " 105851,\n",
       " 105855,\n",
       " 105875,\n",
       " 105877,\n",
       " 105907,\n",
       " 105949,\n",
       " 106022,\n",
       " 106067,\n",
       " 106080,\n",
       " 106121,\n",
       " 106135,\n",
       " 106137,\n",
       " 106148,\n",
       " 106160,\n",
       " 106169,\n",
       " 106177,\n",
       " 106248,\n",
       " 106269,\n",
       " 106281,\n",
       " 106304,\n",
       " 106307,\n",
       " 106308,\n",
       " 106323,\n",
       " 106332,\n",
       " 106337,\n",
       " 106423,\n",
       " 106431,\n",
       " 106432,\n",
       " 106444,\n",
       " 106450,\n",
       " 106474,\n",
       " 106496,\n",
       " 106533,\n",
       " 106534,\n",
       " 106538,\n",
       " 106593,\n",
       " 106655,\n",
       " 106657,\n",
       " 106697,\n",
       " 106699,\n",
       " 106719,\n",
       " 106804,\n",
       " 107007,\n",
       " 107113,\n",
       " 107129,\n",
       " 107139,\n",
       " 107141,\n",
       " 107325,\n",
       " 107357,\n",
       " 107389,\n",
       " 107406,\n",
       " 107411,\n",
       " 107413,\n",
       " 107465,\n",
       " 107471,\n",
       " 107496,\n",
       " 107497,\n",
       " 107500,\n",
       " 107507,\n",
       " 107553,\n",
       " 107554,\n",
       " 107688,\n",
       " 107703,\n",
       " 107806,\n",
       " 107833,\n",
       " 107864,\n",
       " 107956,\n",
       " 107987,\n",
       " 108162,\n",
       " 108240,\n",
       " 108320,\n",
       " 108328,\n",
       " 108329,\n",
       " 108330,\n",
       " 108339,\n",
       " 108358,\n",
       " 108433,\n",
       " 108434,\n",
       " 108465,\n",
       " 108478,\n",
       " 108481,\n",
       " 108489,\n",
       " 108498,\n",
       " 108515,\n",
       " 108544,\n",
       " 108551,\n",
       " 108657,\n",
       " 108694,\n",
       " 108725,\n",
       " 108737,\n",
       " 108758,\n",
       " 108759,\n",
       " 108796,\n",
       " 108806,\n",
       " 108814,\n",
       " 108851,\n",
       " 108856,\n",
       " 108872,\n",
       " 108885,\n",
       " 108895,\n",
       " 108898,\n",
       " 108902,\n",
       " 108904,\n",
       " 108907,\n",
       " 108909,\n",
       " 108912,\n",
       " 108934,\n",
       " 108969,\n",
       " 108973,\n",
       " 108996,\n",
       " 108998,\n",
       " 109009,\n",
       " 109020,\n",
       " 109059,\n",
       " 109098,\n",
       " 109100,\n",
       " 109101,\n",
       " 109106,\n",
       " 109107,\n",
       " 109156,\n",
       " 109233,\n",
       " 109305,\n",
       " 109498,\n",
       " 109582,\n",
       " 109634,\n",
       " 109649,\n",
       " 109727,\n",
       " 109745,\n",
       " 109808,\n",
       " 109822,\n",
       " 109827,\n",
       " 109831,\n",
       " 109888,\n",
       " 109989,\n",
       " 110337,\n",
       " 110338,\n",
       " 110340,\n",
       " 110421,\n",
       " 110424,\n",
       " 110426,\n",
       " 110434,\n",
       " 110464,\n",
       " 110514,\n",
       " 110540,\n",
       " 110549,\n",
       " 110561,\n",
       " 110573,\n",
       " 110594,\n",
       " 110684,\n",
       " 110738,\n",
       " 110744,\n",
       " 110747,\n",
       " 110763,\n",
       " 110938,\n",
       " 110973,\n",
       " 111019,\n",
       " 111023,\n",
       " 111031,\n",
       " 111037,\n",
       " 111044,\n",
       " 111063,\n",
       " 111073,\n",
       " 111212,\n",
       " 111243,\n",
       " 111253,\n",
       " 111269,\n",
       " 111280,\n",
       " 111284,\n",
       " 111289,\n",
       " 111367,\n",
       " 111438,\n",
       " 111469,\n",
       " 111534,\n",
       " 111611,\n",
       " 111656,\n",
       " 111880,\n",
       " 111918,\n",
       " 111959,\n",
       " 112005,\n",
       " 112015,\n",
       " 112031,\n",
       " 112042,\n",
       " 112071,\n",
       " 112082,\n",
       " 112092,\n",
       " 112094,\n",
       " 112129,\n",
       " 112146,\n",
       " 112184,\n",
       " 112259,\n",
       " 112297,\n",
       " 112329,\n",
       " 112439,\n",
       " 112495,\n",
       " 112496,\n",
       " 112505,\n",
       " 112540,\n",
       " 112641,\n",
       " 112704,\n",
       " 112878,\n",
       " 112942,\n",
       " 113001,\n",
       " 113003,\n",
       " 113011,\n",
       " 113020,\n",
       " 113106,\n",
       " 113111,\n",
       " 113210,\n",
       " 113211,\n",
       " 113224,\n",
       " 113234,\n",
       " 113237,\n",
       " 113279,\n",
       " 113396,\n",
       " 113442,\n",
       " 113488,\n",
       " 113505,\n",
       " 113523,\n",
       " 113546,\n",
       " 113624,\n",
       " 113649,\n",
       " 113660,\n",
       " 113698,\n",
       " 113737,\n",
       " 113740,\n",
       " 113774,\n",
       " 113779,\n",
       " 113842,\n",
       " 113896,\n",
       " 113924,\n",
       " 113957,\n",
       " 114011,\n",
       " 114106,\n",
       " 114113,\n",
       " 114131,\n",
       " 114196,\n",
       " 114197,\n",
       " 114237,\n",
       " 114238,\n",
       " 114306,\n",
       " 114387,\n",
       " 114397,\n",
       " 114458,\n",
       " 114469,\n",
       " 114488,\n",
       " 114517,\n",
       " 114522,\n",
       " 114589,\n",
       " 114696,\n",
       " 114728,\n",
       " 114742,\n",
       " 114886,\n",
       " 114887,\n",
       " 114895,\n",
       " 114960,\n",
       " 115065,\n",
       " 115139,\n",
       " 115160,\n",
       " 115190,\n",
       " 115218,\n",
       " 115280,\n",
       " 115314,\n",
       " 115342,\n",
       " 115343,\n",
       " 115443,\n",
       " 115501,\n",
       " 115582,\n",
       " 115602,\n",
       " 115656,\n",
       " 115689,\n",
       " 115719,\n",
       " 115746,\n",
       " 115756,\n",
       " 115772,\n",
       " 115775,\n",
       " 115776,\n",
       " 115979,\n",
       " 116016,\n",
       " 116041,\n",
       " 116149,\n",
       " 116160,\n",
       " 116181,\n",
       " 116192,\n",
       " 116247,\n",
       " 116286,\n",
       " 116453,\n",
       " 116500,\n",
       " 116592,\n",
       " 116642,\n",
       " 116819,\n",
       " 116891,\n",
       " 116912,\n",
       " 117050,\n",
       " 117087,\n",
       " 117110,\n",
       " 117114,\n",
       " 117183,\n",
       " 117227,\n",
       " 117249,\n",
       " 117264,\n",
       " 117271,\n",
       " 117291,\n",
       " 117305,\n",
       " 117363,\n",
       " 117385,\n",
       " 117392,\n",
       " 117396,\n",
       " 117485,\n",
       " 117499,\n",
       " 117557,\n",
       " 117563,\n",
       " 117623,\n",
       " 117624,\n",
       " 117783,\n",
       " 117784,\n",
       " 117898,\n",
       " 117899,\n",
       " 118032,\n",
       " 118054,\n",
       " 118078,\n",
       " 118103,\n",
       " 118117,\n",
       " 118150,\n",
       " 118235,\n",
       " 118326,\n",
       " 118339,\n",
       " 118340,\n",
       " 118395,\n",
       " 118425,\n",
       " 118427,\n",
       " 118493,\n",
       " 118578,\n",
       " 118579,\n",
       " 118582,\n",
       " 118589,\n",
       " 118633,\n",
       " 118663,\n",
       " 118666,\n",
       " 118708,\n",
       " 118805,\n",
       " 118814,\n",
       " 118816,\n",
       " 118863,\n",
       " 118910,\n",
       " 118979,\n",
       " 119036,\n",
       " 119077,\n",
       " 119243,\n",
       " 119268,\n",
       " 119346,\n",
       " 119347,\n",
       " 119348,\n",
       " 119349,\n",
       " 119350,\n",
       " 119351,\n",
       " 119353,\n",
       " 119360,\n",
       " 119474,\n",
       " 119502,\n",
       " 119556,\n",
       " 119575,\n",
       " 119581,\n",
       " 119628,\n",
       " 119663,\n",
       " 119727,\n",
       " 119750,\n",
       " 119751,\n",
       " 120017,\n",
       " 120089,\n",
       " 120124,\n",
       " 120144,\n",
       " 120188,\n",
       " 120195,\n",
       " 120222,\n",
       " 120381,\n",
       " 120471,\n",
       " 120594,\n",
       " 120601,\n",
       " 120643,\n",
       " 120843,\n",
       " 121044,\n",
       " 121061,\n",
       " 121132,\n",
       " 121160,\n",
       " 121169,\n",
       " 121273,\n",
       " 121306,\n",
       " 121314,\n",
       " 121318,\n",
       " 121362,\n",
       " 121498,\n",
       " 121546,\n",
       " 121650,\n",
       " 121673,\n",
       " 121681,\n",
       " 121687,\n",
       " 121840,\n",
       " 121944,\n",
       " 122002,\n",
       " 122006,\n",
       " 122023,\n",
       " 122024,\n",
       " 122043,\n",
       " 122044,\n",
       " 122062,\n",
       " 122063,\n",
       " 122064,\n",
       " 122098,\n",
       " 122123,\n",
       " 122124,\n",
       " 122162,\n",
       " 122224,\n",
       " 122329,\n",
       " 122332,\n",
       " 122333,\n",
       " 122344,\n",
       " 122434,\n",
       " 122543,\n",
       " 122552,\n",
       " 122587,\n",
       " 122754,\n",
       " 122824,\n",
       " 122948,\n",
       " 122960,\n",
       " 123004,\n",
       " 123005,\n",
       " 123036,\n",
       " 123086,\n",
       " 123119,\n",
       " 123130,\n",
       " 123131,\n",
       " 123167,\n",
       " 123170,\n",
       " 123204,\n",
       " 123231,\n",
       " 123308,\n",
       " 123362,\n",
       " 123389,\n",
       " 123403,\n",
       " 123408,\n",
       " 123509,\n",
       " 123520,\n",
       " 123569,\n",
       " 123571,\n",
       " 123575,\n",
       " 123576,\n",
       " 123580,\n",
       " 123613,\n",
       " 123614,\n",
       " 123631,\n",
       " 123681,\n",
       " 123725,\n",
       " 123738,\n",
       " 123799,\n",
       " 123918,\n",
       " 123983,\n",
       " 123986,\n",
       " 123997,\n",
       " 123998,\n",
       " 124005,\n",
       " 124021,\n",
       " 124067,\n",
       " 124068,\n",
       " 124094,\n",
       " 124162,\n",
       " 124239,\n",
       " 124290,\n",
       " 124327,\n",
       " 124565,\n",
       " 124575,\n",
       " 124635,\n",
       " 124655,\n",
       " 124681,\n",
       " 124717,\n",
       " 124731,\n",
       " 124771,\n",
       " 124775,\n",
       " 124791,\n",
       " 124800,\n",
       " 124864,\n",
       " 124888,\n",
       " 124954,\n",
       " 125019,\n",
       " 125156,\n",
       " 125182,\n",
       " 125195,\n",
       " 125210,\n",
       " 125271,\n",
       " 125326,\n",
       " 125361,\n",
       " 125363,\n",
       " 125396,\n",
       " 125430,\n",
       " 125443,\n",
       " 125459,\n",
       " 125528,\n",
       " 125541,\n",
       " 125568,\n",
       " 125714,\n",
       " 125756,\n",
       " 125763,\n",
       " 125847,\n",
       " 125878,\n",
       " 125891,\n",
       " 125892,\n",
       " 125908,\n",
       " 125959,\n",
       " 125980,\n",
       " 125989,\n",
       " 126002,\n",
       " 126025,\n",
       " 126053,\n",
       " 126063,\n",
       " 126122,\n",
       " 126214,\n",
       " 126218,\n",
       " 126297,\n",
       " 126344,\n",
       " 126358,\n",
       " 126387,\n",
       " 126404,\n",
       " 126406,\n",
       " 126421,\n",
       " 126475,\n",
       " 126481,\n",
       " 126502,\n",
       " 126524,\n",
       " 126525,\n",
       " 126610,\n",
       " 126620,\n",
       " 126645,\n",
       " 126694,\n",
       " 126713,\n",
       " 126745,\n",
       " 126976,\n",
       " 127008,\n",
       " 127069,\n",
       " 127127,\n",
       " 127129,\n",
       " 127165,\n",
       " 127174,\n",
       " 127267,\n",
       " 127277,\n",
       " 127283,\n",
       " 127360,\n",
       " 127368,\n",
       " 127488,\n",
       " 127516,\n",
       " 127531,\n",
       " 127616,\n",
       " 127627,\n",
       " 127659,\n",
       " 127677,\n",
       " 127716,\n",
       " 127733,\n",
       " 127792,\n",
       " 127794,\n",
       " 127822,\n",
       " 127910,\n",
       " 128017,\n",
       " 128031,\n",
       " 128042,\n",
       " 128076,\n",
       " 128092,\n",
       " 128184,\n",
       " 128232,\n",
       " 128315,\n",
       " 128323,\n",
       " 128386,\n",
       " 128434,\n",
       " 128476,\n",
       " 128478,\n",
       " 128483,\n",
       " 128517,\n",
       " 128528,\n",
       " 128562,\n",
       " 128603,\n",
       " 128606,\n",
       " 128619,\n",
       " 128807,\n",
       " 128854,\n",
       " 128937,\n",
       " 128948,\n",
       " 128984,\n",
       " 128999,\n",
       " 129121,\n",
       " 129166,\n",
       " 129176,\n",
       " 129191,\n",
       " 129204,\n",
       " 129220,\n",
       " 129248,\n",
       " 129309,\n",
       " 129334,\n",
       " 129337,\n",
       " 129358,\n",
       " 129372,\n",
       " 129379,\n",
       " 129403,\n",
       " 129407,\n",
       " 129432,\n",
       " 129433,\n",
       " 129435,\n",
       " 129438,\n",
       " 129439,\n",
       " 129445,\n",
       " 129454,\n",
       " 129462,\n",
       " 129477,\n",
       " 129491,\n",
       " 129542,\n",
       " 129555,\n",
       " 129570,\n",
       " 129601,\n",
       " 129638,\n",
       " 129643,\n",
       " 129648,\n",
       " 129649,\n",
       " 129655,\n",
       " 129682,\n",
       " 129687,\n",
       " 129696,\n",
       " 129708,\n",
       " 129739,\n",
       " 129746,\n",
       " 129749,\n",
       " 129750,\n",
       " 129753,\n",
       " 129825,\n",
       " 129832,\n",
       " 129837,\n",
       " 129903,\n",
       " 129919,\n",
       " 129946,\n",
       " 129953,\n",
       " 129967,\n",
       " 129970,\n",
       " 130046,\n",
       " 130098,\n",
       " 130117,\n",
       " 130140,\n",
       " 130199,\n",
       " 130200,\n",
       " 130288,\n",
       " 130308,\n",
       " 130316,\n",
       " 130425,\n",
       " 130467,\n",
       " 130469,\n",
       " 130509,\n",
       " 130524,\n",
       " 130529,\n",
       " 130537,\n",
       " 130571,\n",
       " 130593,\n",
       " 130623,\n",
       " 130626,\n",
       " 130668,\n",
       " 130671,\n",
       " 130676,\n",
       " 130695,\n",
       " 130709,\n",
       " 130710,\n",
       " 130721,\n",
       " 130724,\n",
       " 130732,\n",
       " 130793,\n",
       " 130798,\n",
       " 130853,\n",
       " 130993,\n",
       " 131118,\n",
       " 131120,\n",
       " 131131,\n",
       " 131154,\n",
       " 131303,\n",
       " 131319,\n",
       " 131323,\n",
       " 131346,\n",
       " 131398,\n",
       " 131437,\n",
       " 131603,\n",
       " 131647,\n",
       " 131685,\n",
       " 131709,\n",
       " 131738,\n",
       " 131805,\n",
       " 131838,\n",
       " 131872,\n",
       " 131958,\n",
       " 131960,\n",
       " 131974,\n",
       " 131980,\n",
       " 132050,\n",
       " 132112,\n",
       " 132169,\n",
       " 132195,\n",
       " 132200,\n",
       " 132201,\n",
       " 132223,\n",
       " 132294,\n",
       " 132390,\n",
       " 132441,\n",
       " 132456,\n",
       " 132457,\n",
       " 132499,\n",
       " 132543,\n",
       " 132607,\n",
       " 132621,\n",
       " 132634,\n",
       " 132637,\n",
       " 132655,\n",
       " 132661,\n",
       " 132675,\n",
       " 132685,\n",
       " 132785,\n",
       " 132838,\n",
       " 132853,\n",
       " 132858,\n",
       " 132867,\n",
       " 132914,\n",
       " 132979,\n",
       " 132982,\n",
       " 133040,\n",
       " 133051,\n",
       " 133159,\n",
       " 133170,\n",
       " 133224,\n",
       " 133254,\n",
       " 133317,\n",
       " 133319,\n",
       " 133355,\n",
       " 133414,\n",
       " 133466,\n",
       " 133496,\n",
       " 133508,\n",
       " 133534,\n",
       " 133549,\n",
       " 133606,\n",
       " 133645,\n",
       " 133659,\n",
       " 133674,\n",
       " 133707,\n",
       " 133726,\n",
       " 133728,\n",
       " 133738,\n",
       " 133742,\n",
       " 133751,\n",
       " 133756,\n",
       " 133781,\n",
       " 133906,\n",
       " 133983,\n",
       " 133989,\n",
       " 133990,\n",
       " 134018,\n",
       " 134044,\n",
       " 134132,\n",
       " 134164,\n",
       " 134269,\n",
       " 134368,\n",
       " 134486,\n",
       " 134500,\n",
       " 134537,\n",
       " 134595,\n",
       " 134607,\n",
       " 134636,\n",
       " 134693,\n",
       " 134696,\n",
       " 134768,\n",
       " 134770,\n",
       " 134771,\n",
       " 134812,\n",
       " 134850,\n",
       " 134871,\n",
       " 134977,\n",
       " 135102,\n",
       " 135164,\n",
       " 135270,\n",
       " 135340,\n",
       " 135398,\n",
       " 135486,\n",
       " 135581,\n",
       " 135589,\n",
       " 135698,\n",
       " 135704,\n",
       " 135711,\n",
       " 135739,\n",
       " 135772,\n",
       " 135785,\n",
       " 135821,\n",
       " 135903,\n",
       " 135949,\n",
       " 135954,\n",
       " 135979,\n",
       " 136030,\n",
       " 136035,\n",
       " 136057,\n",
       " 136076,\n",
       " 136086,\n",
       " 136224,\n",
       " 136286,\n",
       " 136301,\n",
       " 136321,\n",
       " 136371,\n",
       " 136469,\n",
       " 136594,\n",
       " 136634,\n",
       " 136721,\n",
       " 136770,\n",
       " 136783,\n",
       " 136798,\n",
       " 136804,\n",
       " 136855,\n",
       " 136859,\n",
       " 136941,\n",
       " 136942,\n",
       " 136989,\n",
       " 137021,\n",
       " 137034,\n",
       " 137072,\n",
       " 137112,\n",
       " 137117,\n",
       " 137184,\n",
       " 137217,\n",
       " 137231,\n",
       " 137264,\n",
       " 137307,\n",
       " 137323,\n",
       " 137327,\n",
       " 137329,\n",
       " 137400,\n",
       " 137432,\n",
       " 137447,\n",
       " 137517,\n",
       " 137523,\n",
       " 137581,\n",
       " 137704,\n",
       " 137720,\n",
       " 137834,\n",
       " 137925,\n",
       " 137938,\n",
       " 137942,\n",
       " 137978,\n",
       " 138094,\n",
       " 138169,\n",
       " 138225,\n",
       " 138352,\n",
       " 138401,\n",
       " 138405,\n",
       " 138435,\n",
       " 138438,\n",
       " 138474,\n",
       " 138499,\n",
       " 138550,\n",
       " 138710,\n",
       " 138735,\n",
       " 138774,\n",
       " 138847,\n",
       " 138874,\n",
       " 138880,\n",
       " 138945,\n",
       " 139006,\n",
       " 139022,\n",
       " 139046,\n",
       " 139101,\n",
       " 139157,\n",
       " 139231,\n",
       " 139307,\n",
       " 139387,\n",
       " 139489,\n",
       " 139561,\n",
       " 139623,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lost_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "another_translated_all[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translated_all[75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filename_translated_all, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in translated_all:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(mwmf)):\n",
    "    if mwmf[i] == \"\" and another_mwmf[i] != \"\":\n",
    "        mwmf[i] = another_mwmf[i] \n",
    "        itermax[i] = another_itermax[i]\n",
    "        inter[i] = another_inter[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in mwmf:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in itermax:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "    for line in inter:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "синк доков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mcorpora/subtitles/translations/opus10_spacy_final.bin\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      2\u001b[0m     en_translated_bytes_data \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[1;32m----> 4\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mblank(\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m doc_bin \u001b[39m=\u001b[39m DocBin()\u001b[39m.\u001b[39mfrom_bytes(en_translated_bytes_data)\n\u001b[0;32m      6\u001b[0m en_translated_docs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(doc_bin\u001b[39m.\u001b[39mget_docs(nlp\u001b[39m.\u001b[39mvocab))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"rb\") as file:\n",
    "    en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(en_translated_bytes_data)\n",
    "en_translated_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "len(en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"translations_after_req_words/translations/opus10_spacy_check2.bin\", \"rb\") as file:\n",
    "    another_en_translated_bytes_data = file.read()\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_bytes(another_en_translated_bytes_data)\n",
    "another_en_translated_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "len(another_en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_translated_docs[820])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_nlp_lg = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_translated_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "900000it [00:00, 2368423.31it/s]\n",
      "100%|██████████| 900000/900000 [00:28<00:00, 31274.75it/s]\n"
     ]
    }
   ],
   "source": [
    "empty = en_nlp_lg(\"\")\n",
    "docs = []\n",
    "\n",
    "for i, sentence in tqdm(enumerate(translated_all)):\n",
    "    if sentence == \"\":\n",
    "        docs.append(empty)\n",
    "    elif len(en_translated_docs[i]) != 0:\n",
    "        docs.append(en_translated_docs[i])\n",
    "    else:\n",
    "        spacy_doc = en_nlp_lg(sentence)\n",
    "        docs.append(spacy_doc)\n",
    "\n",
    "doc_bin = DocBin()\n",
    "for doc in tqdm(docs):\n",
    "    doc_bin.add(doc)\n",
    "bytes_data = doc_bin.to_bytes()\n",
    "\n",
    "with open(\"corpora/subtitles/translations/opus10_spacy_final.bin\", \"wb\") as file:\n",
    "    file.write(bytes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_all_docs = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "900000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:44<00:00, 450.22it/s]  \n",
      "100%|██████████| 20000/20000 [03:28<00:00, 96.07it/s] \n",
      " 38%|███▊      | 7649/20000 [01:40<02:42, 76.20it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39m# print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m src, trg \u001b[39m=\u001b[39m (ru_tokens, translated_tokens)\n\u001b[1;32m---> 14\u001b[0m alignments \u001b[39m=\u001b[39m myaligner\u001b[39m.\u001b[39;49mget_word_aligns(src, trg)\n\u001b[0;32m     15\u001b[0m mwmf[i] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m alignments[\u001b[39m\"\u001b[39m\u001b[39mmwmf\u001b[39m\u001b[39m\"\u001b[39m]])\n\u001b[0;32m     16\u001b[0m itermax[i] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m alignments[\u001b[39m\"\u001b[39m\u001b[39mitermax\u001b[39m\u001b[39m\"\u001b[39m]])\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\simalign\\simalign.py:209\u001b[0m, in \u001b[0;36mSentenceAligner.get_word_aligns\u001b[1;34m(self, src_sent, trg_sent)\u001b[0m\n\u001b[0;32m    206\u001b[0m \t\u001b[39mfor\u001b[39;00m i, wlist \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(l2_tokens):\n\u001b[0;32m    207\u001b[0m \t\tl2_b2w_map \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m wlist]\n\u001b[1;32m--> 209\u001b[0m vectors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_loader\u001b[39m.\u001b[39;49mget_embed_list([src_sent, trg_sent])\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    210\u001b[0m vectors \u001b[39m=\u001b[39m [vectors[i, :\u001b[39mlen\u001b[39m(bpe_lists[i])] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]]\n\u001b[0;32m    212\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\simalign\\simalign.py:65\u001b[0m, in \u001b[0;36mEmbeddingLoader.get_embed_list\u001b[1;34m(self, sent_batch)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m \tinputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(sent_batch, is_split_into_words\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))[\u001b[39m\"\u001b[39m\u001b[39mhidden_states\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(hidden):\n\u001b[0;32m     67\u001b[0m \t\u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSpecified to take embeddings from layer \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer\u001b[39m}\u001b[39;00m\u001b[39m, but model has only \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(hidden)\u001b[39m}\u001b[39;00m\u001b[39m layers.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:495\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    492\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    493\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    494\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[0;32m    496\u001b[0m         hidden_states,\n\u001b[0;32m    497\u001b[0m         attention_mask,\n\u001b[0;32m    498\u001b[0m         head_mask,\n\u001b[0;32m    499\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    500\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    504\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    416\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    417\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    424\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 425\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[0;32m    426\u001b[0m         hidden_states,\n\u001b[0;32m    427\u001b[0m         attention_mask,\n\u001b[0;32m    428\u001b[0m         head_mask,\n\u001b[0;32m    429\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    430\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    431\u001b[0m         past_key_value,\n\u001b[0;32m    432\u001b[0m         output_attentions,\n\u001b[0;32m    433\u001b[0m     )\n\u001b[0;32m    434\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[0;32m    435\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:284\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m    275\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    276\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    283\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m--> 284\u001b[0m     mixed_query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery(hidden_states)\n\u001b[0;32m    286\u001b[0m     \u001b[39m# If this is instantiated as a cross-attention module, the keys\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39m# and values come from an encoder; the attention mask needs to be\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[39m# such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     is_cross_attention \u001b[39m=\u001b[39m encoder_hidden_states \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\warri\\PycharmProjects\\translational_variability\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chunk_size = 20000\n",
    "latest_chunk = 720000\n",
    "for chunk_start in range(latest_chunk, 800000, chunk_size):\n",
    "\n",
    "    for i in tqdm(range(chunk_start, chunk_start + chunk_size)):\n",
    "        if translated_all[i] == \"\":\n",
    "            continue\n",
    "        if mwmf[i] != \"\":\n",
    "            continue\n",
    "        ru_tokens = [token.text for token in ru_all_docs[i]]\n",
    "        translated_tokens = [token.text for token in translated_all_docs[i]]\n",
    "        # print(f\"{ru_tokens}\\n{translated_tokens}\\n\")\n",
    "        src, trg = (ru_tokens, translated_tokens)\n",
    "        alignments = myaligner.get_word_aligns(src, trg)\n",
    "        mwmf[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"mwmf\"]])\n",
    "        itermax[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"itermax\"]])\n",
    "        inter[i] = \" \".join([f\"{x}-{y}\" for x, y in alignments[\"inter\"]])\n",
    "\n",
    "    with open(fname_mwmf, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in mwmf:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_itermax, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in itermax:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "\n",
    "    with open(fname_inter, 'w', encoding=\"utf-8\") as f:\n",
    "        for line in inter:\n",
    "            f.write(line)\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
